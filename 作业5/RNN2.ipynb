{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f019b473-4573-4dfd-8085-d2ab79a20921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "          or the UNK index if token isn't present.\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "              for the UNK functionality\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "\n",
    "class SurnameVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "\n",
    "    def __init__(self, char_vocab, nationality_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            char_vocab (SequenceVocabulary): maps words to integers\n",
    "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "        \"\"\"\n",
    "        self.char_vocab = char_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname, vector_length=-1):\n",
    "        \"\"\"Vectorize a surname into a vector of observations and targets\n",
    "\n",
    "        The outputs are the vectorized surname split into two vectors:\n",
    "            surname[:-1] and surname[1:]\n",
    "        At each timestep, the first vector is the observation and the second vector is the target.\n",
    "\n",
    "        Args:\n",
    "            surname (str): the surname to be vectorized\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            a tuple: (from_vector, to_vector)\n",
    "            from_vector (numpy.ndarray): the observation vector\n",
    "            to_vector (numpy.ndarray): the target prediction vector\n",
    "        \"\"\"\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices) - 1\n",
    "\n",
    "        from_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        from_indices = indices[:-1]\n",
    "        from_vector[:len(from_indices)] = from_indices\n",
    "        from_vector[len(from_indices):] = self.char_vocab.mask_index\n",
    "\n",
    "        to_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        to_indices = indices[1:]\n",
    "        to_vector[:len(to_indices)] = to_indices\n",
    "        to_vector[len(to_indices):] = self.char_vocab.mask_index\n",
    "\n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the surname dataset\n",
    "        Returns:\n",
    "            an instance of the SurnameVectorizer\n",
    "        \"\"\"\n",
    "        char_vocab = SequenceVocabulary()\n",
    "        nationality_vocab = Vocabulary()\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for char in row.surname:\n",
    "                char_vocab.add_token(char)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(char_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Instantiate the vectorizer from saved contents\n",
    "\n",
    "        Args:\n",
    "            contents (dict): a dict holding two vocabularies for this vectorizer\n",
    "                This dictionary is created using `vectorizer.to_serializable()`\n",
    "        Returns:\n",
    "            an instance of SurnameVectorizer\n",
    "        \"\"\"\n",
    "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
    "        nat_vocab = Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "\n",
    "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" Returns the serializable contents \"\"\"\n",
    "        return {'char_vocab': self.char_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34706392-b1c3-403e-9125-9cdce5323592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch7/model2_conditioned_surname_generation\\vectorizer.json\n",
      "\tmodel_storage/ch7/model2_conditioned_surname_generation\\model.pth\n",
      "Using CUDA: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training routine:   0%|                                                                         | 0/50 [00:00<?, ?it/s]\n",
      "split=train:   0%|                                                                              | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "split=val:   0%|                                                                                | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "split=train:   0%|                                               | 0/60 [00:00<?, ?it/s, acc=0.815, epoch=0, loss=4.54]\u001b[A\n",
      "split=train:   7%|██▌                                    | 4/60 [00:00<00:01, 31.25it/s, acc=0.815, epoch=0, loss=4.54]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  13%|█████▏                                 | 8/60 [00:00<00:01, 34.07it/s, acc=0.815, epoch=0, loss=4.54]\u001b[A\n",
      "split=train:  22%|████████▏                             | 13/60 [00:00<00:01, 37.29it/s, acc=0.815, epoch=0, loss=4.54]\u001b[A\n",
      "split=train:  28%|██████████▊                           | 17/60 [00:00<00:01, 37.25it/s, acc=0.815, epoch=0, loss=4.54]\u001b[A\n",
      "split=train:  35%|█████████████▎                        | 21/60 [00:00<00:01, 37.89it/s, acc=0.815, epoch=0, loss=4.54]\u001b[A\n",
      "split=train:  42%|████████████████▎                      | 25/60 [00:00<00:00, 37.89it/s, acc=3.37, epoch=0, loss=4.38]\u001b[A\n",
      "split=train:  43%|████████████████▉                      | 26/60 [00:00<00:00, 38.67it/s, acc=3.37, epoch=0, loss=4.38]\u001b[A\n",
      "split=train:  50%|███████████████████▌                   | 30/60 [00:00<00:00, 37.94it/s, acc=3.37, epoch=0, loss=4.38]\u001b[A\n",
      "split=train:  57%|██████████████████████                 | 34/60 [00:00<00:00, 38.10it/s, acc=3.37, epoch=0, loss=4.38]\u001b[A\n",
      "split=train:  65%|█████████████████████████▎             | 39/60 [00:01<00:00, 39.10it/s, acc=3.37, epoch=0, loss=4.38]\u001b[A\n",
      "split=train:  72%|███████████████████████████▉           | 43/60 [00:01<00:00, 38.56it/s, acc=3.37, epoch=0, loss=4.38]\u001b[A\n",
      "split=train:  78%|██████████████████████████████▌        | 47/60 [00:01<00:00, 36.59it/s, acc=3.37, epoch=0, loss=4.38]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:01<00:00, 36.59it/s, acc=7.51, epoch=0, loss=4.14]\u001b[A\n",
      "split=train:  85%|█████████████████████████████████▏     | 51/60 [00:01<00:00, 36.04it/s, acc=7.51, epoch=0, loss=4.14]\u001b[A\n",
      "split=train:  92%|███████████████████████████████████▊   | 55/60 [00:01<00:00, 36.85it/s, acc=7.51, epoch=0, loss=4.14]\u001b[A\n",
      "split=train:  98%|██████████████████████████████████████▎| 59/60 [00:01<00:00, 37.11it/s, acc=7.51, epoch=0, loss=4.14]\u001b[A\n",
      "split=val:   0%|                                                  | 0/12 [00:01<?, ?it/s, acc=15.1, epoch=0, loss=3.37]\u001b[A\n",
      "split=val:   8%|███▌                                      | 1/12 [00:01<00:17,  1.63s/it, acc=15.1, epoch=0, loss=3.37]\u001b[A\n",
      "training routine:   2%|█▎                                                               | 1/50 [00:01<01:31,  1.87s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/60 [00:01<00:01, 37.11it/s, acc=13.2, epoch=1, loss=3.4]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|█████████████████                        | 25/60 [00:02<00:00, 37.11it/s, acc=15, epoch=1, loss=3.29]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:03<00:00, 37.11it/s, acc=15.5, epoch=1, loss=3.23]\u001b[A\n",
      "split=val:   0%|                                          | 0/12 [00:03<00:02,  5.32it/s, acc=18.5, epoch=1, loss=3.11]\u001b[A\n",
      "training routine:   4%|██▌                                                              | 2/50 [00:03<01:29,  1.86s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:03<00:01, 37.11it/s, acc=18.8, epoch=2, loss=3.08]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|█████████████████                        | 25/60 [00:04<00:00, 37.11it/s, acc=18, epoch=2, loss=3.06]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:05<00:00, 37.11it/s, acc=18.2, epoch=2, loss=3.03]\u001b[A\n",
      "training routine:   6%|███▉                                                             | 3/50 [00:05<01:25,  1.82s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:05<00:01, 37.11it/s, acc=19.5, epoch=3, loss=2.99]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▎                      | 25/60 [00:06<00:00, 37.11it/s, acc=19.1, epoch=3, loss=2.93]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:06<00:00, 37.11it/s, acc=19.7, epoch=3, loss=2.91]\u001b[A\n",
      "training routine:   8%|█████▏                                                           | 4/50 [00:07<01:21,  1.77s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:07<00:01, 37.11it/s, acc=20.8, epoch=4, loss=2.87]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▎                      | 25/60 [00:07<00:00, 37.11it/s, acc=21.1, epoch=4, loss=2.83]\u001b[A\n",
      "split=train:  83%|█████████████████████████████████▎      | 50/60 [00:08<00:00, 37.11it/s, acc=21.5, epoch=4, loss=2.8]\u001b[A\n",
      "training routine:  10%|██████▌                                                          | 5/50 [00:08<01:18,  1.74s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:08<00:01, 37.11it/s, acc=21.9, epoch=5, loss=2.77]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▎                      | 25/60 [00:09<00:00, 37.11it/s, acc=22.2, epoch=5, loss=2.75]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:10<00:00, 37.11it/s, acc=22.7, epoch=5, loss=2.74]\u001b[A\n",
      "training routine:  12%|███████▊                                                         | 6/50 [00:10<01:17,  1.76s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:10<00:01, 37.11it/s, acc=22.5, epoch=6, loss=2.73]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▋                       | 25/60 [00:11<00:00, 37.11it/s, acc=23.3, epoch=6, loss=2.7]\u001b[A\n",
      "split=train:  83%|█████████████████████████████████▎      | 50/60 [00:12<00:00, 37.11it/s, acc=23.4, epoch=6, loss=2.7]\u001b[A\n",
      "training routine:  14%|█████████                                                        | 7/50 [00:12<01:16,  1.78s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:12<00:01, 37.11it/s, acc=23.9, epoch=7, loss=2.65]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|█████████████████                        | 25/60 [00:13<00:00, 37.11it/s, acc=24, epoch=7, loss=2.67]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:13<00:00, 37.11it/s, acc=24.2, epoch=7, loss=2.66]\u001b[A\n",
      "training routine:  16%|██████████▍                                                      | 8/50 [00:14<01:15,  1.79s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:14<00:01, 37.11it/s, acc=23.7, epoch=8, loss=2.69]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▎                      | 25/60 [00:14<00:00, 37.11it/s, acc=24.6, epoch=8, loss=2.63]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:15<00:00, 37.11it/s, acc=24.6, epoch=8, loss=2.63]\u001b[A\n",
      "training routine:  18%|███████████▋                                                     | 9/50 [00:15<01:12,  1.76s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:16<00:01, 37.11it/s, acc=24.9, epoch=9, loss=2.59]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▎                      | 25/60 [00:16<00:00, 37.11it/s, acc=24.7, epoch=9, loss=2.61]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:17<00:00, 37.11it/s, acc=24.9, epoch=9, loss=2.61]\u001b[A\n",
      "training routine:  20%|████████████▊                                                   | 10/50 [00:17<01:09,  1.74s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/60 [00:17<00:01, 37.11it/s, acc=26, epoch=10, loss=2.61]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [11/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:18<00:00, 37.11it/s, acc=25.5, epoch=10, loss=2.59]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:18<00:00, 37.11it/s, acc=25.4, epoch=10, loss=2.59]\u001b[A\n",
      "training routine:  22%|██████████████                                                  | 11/50 [00:19<01:07,  1.73s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:19<00:01, 37.11it/s, acc=23.1, epoch=11, loss=2.64]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [12/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  40%|███████████████▏                      | 24/60 [00:20<00:00, 37.11it/s, acc=23.1, epoch=11, loss=2.64]\u001b[A\n",
      "split=val:   0%|                                          | 0/12 [00:20<00:03,  3.09it/s, acc=25.9, epoch=10, loss=2.6]\u001b[A\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:20<00:00, 37.11it/s, acc=25.5, epoch=11, loss=2.58]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:20<00:00, 37.11it/s, acc=25.7, epoch=11, loss=2.58]\u001b[A\n",
      "split=train: 100%|██████████████████████████████████████| 60/60 [00:20<00:00,  1.87s/it, acc=25.7, epoch=11, loss=2.58]\u001b[A\n",
      "split=val:   0%|                                         | 0/12 [00:20<00:03,  3.09it/s, acc=27.7, epoch=11, loss=2.56]\u001b[A\n",
      "training routine:  24%|███████████████▎                                                | 12/50 [00:21<01:05,  1.72s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:21<01:52,  1.87s/it, acc=23.1, epoch=12, loss=2.65]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [13/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:21<01:05,  1.87s/it, acc=26.1, epoch=12, loss=2.56]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:22<00:18,  1.87s/it, acc=26.1, epoch=12, loss=2.56]\u001b[A\n",
      "training routine:  26%|████████████████▋                                               | 13/50 [00:23<01:05,  1.78s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:23<01:52,  1.87s/it, acc=25.5, epoch=13, loss=2.55]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [14/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:23<01:05,  1.87s/it, acc=26.2, epoch=13, loss=2.55]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:24<00:18,  1.87s/it, acc=26.4, epoch=13, loss=2.55]\u001b[A\n",
      "training routine:  28%|█████████████████▉                                              | 14/50 [00:24<01:03,  1.76s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:24<01:52,  1.87s/it, acc=26.6, epoch=14, loss=2.55]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [15/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:25<01:05,  1.87s/it, acc=26.6, epoch=14, loss=2.53]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:25<00:18,  1.87s/it, acc=26.4, epoch=14, loss=2.54]\u001b[A\n",
      "training routine:  30%|███████████████████▏                                            | 15/50 [00:26<01:00,  1.73s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:26<01:52,  1.87s/it, acc=26.2, epoch=15, loss=2.51]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [16/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:27<01:05,  1.87s/it, acc=26.3, epoch=15, loss=2.53]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:27<00:18,  1.87s/it, acc=26.6, epoch=15, loss=2.53]\u001b[A\n",
      "training routine:  32%|████████████████████▍                                           | 16/50 [00:28<00:58,  1.71s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:28<01:52,  1.87s/it, acc=28.5, epoch=16, loss=2.46]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [17/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:28<01:05,  1.87s/it, acc=26.5, epoch=16, loss=2.53]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:29<00:18,  1.87s/it, acc=26.8, epoch=16, loss=2.52]\u001b[A\n",
      "training routine:  34%|█████████████████████▊                                          | 17/50 [00:29<00:55,  1.69s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/60 [00:29<01:52,  1.87s/it, acc=27, epoch=17, loss=2.51]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [18/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:30<01:05,  1.87s/it, acc=26.6, epoch=17, loss=2.53]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:30<00:18,  1.87s/it, acc=26.9, epoch=17, loss=2.51]\u001b[A\n",
      "training routine:  36%|███████████████████████                                         | 18/50 [00:31<00:53,  1.68s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:31<01:52,  1.87s/it, acc=24.7, epoch=18, loss=2.57]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [19/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:31<01:05,  1.87s/it, acc=26.9, epoch=18, loss=2.51]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:32<00:18,  1.87s/it, acc=27.1, epoch=18, loss=2.51]\u001b[A\n",
      "training routine:  38%|████████████████████████▎                                       | 19/50 [00:32<00:51,  1.67s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:33<01:52,  1.87s/it, acc=26.1, epoch=19, loss=2.57]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [20/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▎                      | 25/60 [00:33<01:05,  1.87s/it, acc=27.6, epoch=19, loss=2.5]\u001b[A\n",
      "split=train:  83%|████████████████████████████████▌      | 50/60 [00:34<00:18,  1.87s/it, acc=27.5, epoch=19, loss=2.5]\u001b[A\n",
      "training routine:  40%|█████████████████████████▌                                      | 20/50 [00:34<00:49,  1.66s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:34<01:52,  1.87s/it, acc=25.5, epoch=20, loss=2.53]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [21/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:35<01:05,  1.87s/it, acc=27.4, epoch=20, loss=2.49]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:35<00:18,  1.87s/it, acc=27.5, epoch=20, loss=2.49]\u001b[A\n",
      "training routine:  42%|██████████████████████████▉                                     | 21/50 [00:36<00:48,  1.66s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:36<01:52,  1.87s/it, acc=28.3, epoch=21, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [22/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:36<01:05,  1.87s/it, acc=27.6, epoch=21, loss=2.49]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:37<00:18,  1.87s/it, acc=27.5, epoch=21, loss=2.49]\u001b[A\n",
      "training routine:  44%|████████████████████████████▏                                   | 22/50 [00:38<00:46,  1.68s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:38<01:52,  1.87s/it, acc=28.5, epoch=22, loss=2.45]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [23/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:38<01:05,  1.87s/it, acc=27.5, epoch=22, loss=2.48]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:39<00:18,  1.87s/it, acc=27.5, epoch=22, loss=2.48]\u001b[A\n",
      "training routine:  46%|█████████████████████████████▍                                  | 23/50 [00:39<00:46,  1.72s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:39<01:52,  1.87s/it, acc=29.1, epoch=23, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [24/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:40<01:05,  1.87s/it, acc=27.7, epoch=23, loss=2.48]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:41<00:18,  1.87s/it, acc=27.6, epoch=23, loss=2.47]\u001b[A\n",
      "training routine:  48%|██████████████████████████████▋                                 | 24/50 [00:41<00:44,  1.70s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:41<01:52,  1.87s/it, acc=27.9, epoch=24, loss=2.51]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [25/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▋                       | 25/60 [00:42<01:05,  1.87s/it, acc=28, epoch=24, loss=2.47]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:42<00:18,  1.87s/it, acc=27.8, epoch=24, loss=2.47]\u001b[A\n",
      "training routine:  50%|████████████████████████████████                                | 25/50 [00:43<00:42,  1.71s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:43<01:52,  1.87s/it, acc=27.7, epoch=25, loss=2.48]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [26/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:43<01:05,  1.87s/it, acc=27.7, epoch=25, loss=2.47]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:44<00:18,  1.87s/it, acc=27.8, epoch=25, loss=2.47]\u001b[A\n",
      "training routine:  52%|█████████████████████████████████▎                              | 26/50 [00:45<00:42,  1.76s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/60 [00:45<01:52,  1.87s/it, acc=28, epoch=26, loss=2.45]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [27/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:45<01:05,  1.87s/it, acc=27.9, epoch=26, loss=2.46]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:46<00:18,  1.87s/it, acc=27.9, epoch=26, loss=2.46]\u001b[A\n",
      "training routine:  54%|██████████████████████████████████▌                             | 27/50 [00:46<00:40,  1.78s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/60 [00:46<01:52,  1.87s/it, acc=28, epoch=27, loss=2.45]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [28/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:47<01:05,  1.87s/it, acc=27.9, epoch=27, loss=2.47]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:48<00:18,  1.87s/it, acc=27.9, epoch=27, loss=2.46]\u001b[A\n",
      "training routine:  56%|███████████████████████████████████▊                            | 28/50 [00:48<00:38,  1.74s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:48<01:52,  1.87s/it, acc=30.4, epoch=28, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [29/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:49<01:05,  1.87s/it, acc=28.5, epoch=28, loss=2.45]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:49<00:18,  1.87s/it, acc=28.4, epoch=28, loss=2.45]\u001b[A\n",
      "training routine:  58%|█████████████████████████████████████                           | 29/50 [00:50<00:35,  1.71s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:50<01:52,  1.87s/it, acc=29.7, epoch=29, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [30/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:50<01:05,  1.87s/it, acc=28.1, epoch=29, loss=2.46]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:51<00:18,  1.87s/it, acc=28.4, epoch=29, loss=2.45]\u001b[A\n",
      "training routine:  60%|██████████████████████████████████████▍                         | 30/50 [00:51<00:33,  1.69s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:51<01:52,  1.87s/it, acc=27.8, epoch=30, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [31/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▋                       | 25/60 [00:52<01:05,  1.87s/it, acc=28, epoch=30, loss=2.44]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:53<00:18,  1.87s/it, acc=28.1, epoch=30, loss=2.45]\u001b[A\n",
      "training routine:  62%|███████████████████████████████████████▋                        | 31/50 [00:53<00:31,  1.68s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:53<01:52,  1.87s/it, acc=29.1, epoch=31, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [32/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:54<01:05,  1.87s/it, acc=28.7, epoch=31, loss=2.45]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:54<00:18,  1.87s/it, acc=28.6, epoch=31, loss=2.44]\u001b[A\n",
      "training routine:  64%|████████████████████████████████████████▉                       | 32/50 [00:55<00:29,  1.67s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [00:55<01:52,  1.87s/it, acc=31.3, epoch=32, loss=2.42]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [33/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:55<01:05,  1.87s/it, acc=28.7, epoch=32, loss=2.44]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:56<00:18,  1.87s/it, acc=28.4, epoch=32, loss=2.45]\u001b[A\n",
      "training routine:  66%|██████████████████████████████████████████▏                     | 33/50 [00:56<00:28,  1.68s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/60 [00:56<01:52,  1.87s/it, acc=27, epoch=33, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [34/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:57<01:05,  1.87s/it, acc=28.2, epoch=33, loss=2.44]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:58<00:18,  1.87s/it, acc=28.6, epoch=33, loss=2.43]\u001b[A\n",
      "training routine:  68%|███████████████████████████████████████████▌                    | 34/50 [00:58<00:27,  1.70s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [00:58<01:52,  1.87s/it, acc=28.8, epoch=34, loss=2.4]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [35/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [00:59<01:05,  1.87s/it, acc=28.3, epoch=34, loss=2.45]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [00:59<00:18,  1.87s/it, acc=28.5, epoch=34, loss=2.44]\u001b[A\n",
      "training routine:  70%|████████████████████████████████████████████▊                   | 35/50 [01:00<00:25,  1.70s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:00<01:52,  1.87s/it, acc=26.2, epoch=35, loss=2.49]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [36/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:00<01:05,  1.87s/it, acc=28.3, epoch=35, loss=2.44]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:01<00:18,  1.87s/it, acc=28.7, epoch=35, loss=2.43]\u001b[A\n",
      "training routine:  72%|██████████████████████████████████████████████                  | 36/50 [01:01<00:23,  1.69s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/60 [01:01<01:52,  1.87s/it, acc=27, epoch=36, loss=2.42]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [37/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:02<01:05,  1.87s/it, acc=28.4, epoch=36, loss=2.44]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:03<00:18,  1.87s/it, acc=28.6, epoch=36, loss=2.43]\u001b[A\n",
      "training routine:  74%|███████████████████████████████████████████████▎                | 37/50 [01:03<00:21,  1.67s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:03<01:52,  1.87s/it, acc=27.7, epoch=37, loss=2.46]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [38/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:04<01:05,  1.87s/it, acc=28.3, epoch=37, loss=2.43]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:04<00:18,  1.87s/it, acc=28.6, epoch=37, loss=2.43]\u001b[A\n",
      "training routine:  76%|████████████████████████████████████████████████▋               | 38/50 [01:05<00:19,  1.67s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:05<01:52,  1.87s/it, acc=26.8, epoch=38, loss=2.49]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [39/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:05<01:05,  1.87s/it, acc=28.4, epoch=38, loss=2.43]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:06<00:18,  1.87s/it, acc=28.7, epoch=38, loss=2.43]\u001b[A\n",
      "training routine:  78%|█████████████████████████████████████████████████▉              | 39/50 [01:06<00:18,  1.66s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [01:06<01:52,  1.87s/it, acc=31.1, epoch=39, loss=2.4]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [40/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:07<01:05,  1.87s/it, acc=29.6, epoch=39, loss=2.41]\u001b[A\n",
      "split=train:  83%|█████████████████████████████████▎      | 50/60 [01:08<00:18,  1.87s/it, acc=29, epoch=39, loss=2.42]\u001b[A\n",
      "training routine:  80%|███████████████████████████████████████████████████▏            | 40/50 [01:08<00:16,  1.69s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:08<01:52,  1.87s/it, acc=28.4, epoch=40, loss=2.47]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [41/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▋                       | 25/60 [01:09<01:05,  1.87s/it, acc=29, epoch=40, loss=2.43]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:09<00:18,  1.87s/it, acc=28.9, epoch=40, loss=2.43]\u001b[A\n",
      "training routine:  82%|████████████████████████████████████████████████████▍           | 41/50 [01:10<00:15,  1.70s/it]\u001b[A\n",
      "split=train:   0%|                                         | 0/60 [01:10<01:52,  1.87s/it, acc=28, epoch=41, loss=2.45]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [42/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:11<01:05,  1.87s/it, acc=28.5, epoch=41, loss=2.42]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:11<00:18,  1.87s/it, acc=28.6, epoch=41, loss=2.42]\u001b[A\n",
      "training routine:  84%|█████████████████████████████████████████████████████▊          | 42/50 [01:12<00:13,  1.69s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:12<01:52,  1.87s/it, acc=27.4, epoch=42, loss=2.46]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [43/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:12<01:05,  1.87s/it, acc=28.5, epoch=42, loss=2.43]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:13<00:18,  1.87s/it, acc=28.7, epoch=42, loss=2.42]\u001b[A\n",
      "training routine:  86%|███████████████████████████████████████████████████████         | 43/50 [01:13<00:11,  1.68s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:13<01:52,  1.87s/it, acc=28.4, epoch=43, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [44/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:14<01:05,  1.87s/it, acc=29.1, epoch=43, loss=2.42]\u001b[A\n",
      "split=train:  83%|█████████████████████████████████▎      | 50/60 [01:14<00:18,  1.87s/it, acc=29, epoch=43, loss=2.42]\u001b[A\n",
      "training routine:  88%|████████████████████████████████████████████████████████▎       | 44/50 [01:15<00:10,  1.69s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:15<01:52,  1.87s/it, acc=29.3, epoch=44, loss=2.41]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [45/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:16<01:05,  1.87s/it, acc=28.8, epoch=44, loss=2.42]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:16<00:18,  1.87s/it, acc=28.8, epoch=44, loss=2.42]\u001b[A\n",
      "training routine:  90%|█████████████████████████████████████████████████████████▌      | 45/50 [01:17<00:08,  1.70s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:17<01:52,  1.87s/it, acc=28.5, epoch=45, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [46/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:17<01:05,  1.87s/it, acc=28.7, epoch=45, loss=2.42]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:18<00:18,  1.87s/it, acc=28.7, epoch=45, loss=2.42]\u001b[A\n",
      "training routine:  92%|██████████████████████████████████████████████████████████▉     | 46/50 [01:18<00:06,  1.71s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:18<01:52,  1.87s/it, acc=27.7, epoch=46, loss=2.43]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [47/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|███████████████▊                      | 25/60 [01:19<01:05,  1.87s/it, acc=28.7, epoch=46, loss=2.42]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:20<00:18,  1.87s/it, acc=28.7, epoch=46, loss=2.42]\u001b[A\n",
      "training routine:  94%|████████████████████████████████████████████████████████████▏   | 47/50 [01:20<00:05,  1.71s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:20<01:52,  1.87s/it, acc=27.8, epoch=47, loss=2.48]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [48/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▋                       | 25/60 [01:21<01:05,  1.87s/it, acc=29, epoch=47, loss=2.41]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:21<00:18,  1.87s/it, acc=28.8, epoch=47, loss=2.42]\u001b[A\n",
      "training routine:  96%|█████████████████████████████████████████████████████████████▍  | 48/50 [01:22<00:03,  1.72s/it]\u001b[A\n",
      "split=train:   0%|                                        | 0/60 [01:22<01:52,  1.87s/it, acc=28.6, epoch=48, loss=2.4]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [49/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▋                       | 25/60 [01:22<01:05,  1.87s/it, acc=29, epoch=48, loss=2.41]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:23<00:18,  1.87s/it, acc=28.8, epoch=48, loss=2.42]\u001b[A\n",
      "training routine:  98%|██████████████████████████████████████████████████████████████▋ | 49/50 [01:24<00:01,  1.71s/it]\u001b[A\n",
      "split=train:   0%|                                       | 0/60 [01:24<01:52,  1.87s/it, acc=29.8, epoch=49, loss=2.38]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [50/50]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "split=train:  42%|████████████████▋                       | 25/60 [01:24<01:05,  1.87s/it, acc=29, epoch=49, loss=2.41]\u001b[A\n",
      "split=train:  83%|███████████████████████████████▋      | 50/60 [01:25<00:18,  1.87s/it, acc=28.9, epoch=49, loss=2.41]\u001b[A\n",
      "training routine: 100%|████████████████████████████████████████████████████████████████| 50/50 [01:25<00:00,  1.70s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (SurnameVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "\n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer.\n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "\n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dictionary holding the data point: (x_data, y_target, class_index)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        from_vector, to_vector = \\\n",
    "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
    "\n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_data': from_vector,\n",
    "                'y_target': to_vector,\n",
    "                'class_index': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "\n",
    "class SurnameGenerationModel(nn.Module):\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities,\n",
    "                 rnn_hidden_size, batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            char_embedding_size (int): The size of the character embeddings\n",
    "            char_vocab_size (int): The number of characters to embed\n",
    "            num_nationalities (int): The size of the prediction vector\n",
    "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
    "            batch_first (bool): Informs whether the input tensors will\n",
    "                have batch or the sequence on the 0th dimension\n",
    "            padding_idx (int): The index for the tensor padding;\n",
    "                see torch.nn.Embedding\n",
    "            dropout_p (float): the probability of zeroing activations using\n",
    "                the dropout method.  higher means more likely to zero.\n",
    "        \"\"\"\n",
    "        super(SurnameGenerationModel, self).__init__()\n",
    "\n",
    "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                     embedding_dim=char_embedding_size,\n",
    "                                     padding_idx=padding_idx)\n",
    "\n",
    "        self.nation_emb = nn.Embedding(num_embeddings=num_nationalities,\n",
    "                                       embedding_dim=rnn_hidden_size)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size,\n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
    "                            out_features=char_vocab_size)\n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, nationality_index, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor.\n",
    "                x_in.shape should be (batch, max_seq_size)\n",
    "            nationality_index (torch.Tensor): The index of the nationality for each data point\n",
    "                Used to initialize the hidden state of the RNN\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\n",
    "        \"\"\"\n",
    "        x_embedded = self.char_emb(x_in)\n",
    "\n",
    "        # hidden_size: (num_layers * num_directions, batch_size, rnn_hidden_size)\n",
    "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\n",
    "\n",
    "        y_out, _ = self.rnn(x_embedded, nationality_embedded)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "\n",
    "        return y_out\n",
    "\n",
    "\n",
    "def sample_from_model(model, vectorizer, nationalities, sample_size=20,\n",
    "                      temperature=1.0):\n",
    "    \"\"\"Sample a sequence of indices from the model\n",
    "\n",
    "    Args:\n",
    "        model (SurnameGenerationModel): the trained model\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "        nationalities (list): a list of integers representing nationalities\n",
    "        sample_size (int): the max length of the samples\n",
    "        temperature (float): accentuates or flattens\n",
    "            the distribution.\n",
    "            0.0 < temperature < 1.0 will make it peakier.\n",
    "            temperature > 1.0 will make it more uniform\n",
    "    Returns:\n",
    "        indices (torch.Tensor): the matrix of indices;\n",
    "        shape = (num_samples, sample_size)\n",
    "    \"\"\"\n",
    "    num_samples = len(nationalities)\n",
    "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index\n",
    "                       for _ in range(num_samples)]\n",
    "    begin_seq_index = torch.tensor(begin_seq_index,\n",
    "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_seq_index]\n",
    "    nationality_indices = torch.tensor(nationalities, dtype=torch.int64).unsqueeze(dim=0)\n",
    "    h_t = model.nation_emb(nationality_indices)\n",
    "\n",
    "    for time_step in range(sample_size):\n",
    "        x_t = indices[time_step]\n",
    "        x_emb_t = model.char_emb(x_t)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    \"\"\"Transform indices into the string form of a surname\n",
    "\n",
    "    Args:\n",
    "        sampled_indices (torch.Tensor): the inidces from `sample_from_model`\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "    \"\"\"\n",
    "    decoded_surnames = []\n",
    "    vocab = vectorizer.char_vocab\n",
    "\n",
    "    for sample_index in range(sampled_indices.shape[0]):\n",
    "        surname = \"\"\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            sample_item = sampled_indices[sample_index, time_step].item()\n",
    "            if sample_item == vocab.begin_seq_index:\n",
    "                continue\n",
    "            elif sample_item == vocab.end_seq_index:\n",
    "                break\n",
    "            else:\n",
    "                surname += vocab.lookup_index(sample_item)\n",
    "        decoded_surnames.append(surname)\n",
    "    return decoded_surnames\n",
    "\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"Normalize tensor sizes\n",
    "\n",
    "    Args:\n",
    "        y_pred (torch.Tensor): the output of the model\n",
    "            If a 3-dimensional tensor, reshapes to a matrix\n",
    "        y_true (torch.Tensor): the target predictions\n",
    "            If a matrix, reshapes to be a vector\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "\n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch7/model2_conditioned_surname_generation\",\n",
    "    # Model hyper parameters\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=50,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
    "                               char_vocab_size=len(vectorizer.char_vocab),\n",
    "                               num_nationalities=len(vectorizer.nationality_vocab),\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,\n",
    "                               padding_idx=vectorizer.char_vocab.mask_index,\n",
    "                               dropout_p=0.5)\n",
    "\n",
    "mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine',\n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size),\n",
    "                          position=1,\n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size),\n",
    "                        position=1,\n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        print(f\"\\nEpoch [{epoch_index + 1}/{args.num_epochs}]\")\n",
    "        print(\"-\" * 50)\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = model(x_in=batch_dict['x_data'],\n",
    "                           nationality_index=batch_dict['class_index'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            if batch_index % 25 == 0:\n",
    "                train_bar.set_postfix(loss=running_loss,\n",
    "                                      acc=running_acc,\n",
    "                                      epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = model(x_in=batch_dict['x_data'],\n",
    "                           nationality_index=batch_dict['class_index'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # Update bar\n",
    "            if batch_index % 25 == 0:\n",
    "                val_bar.set_postfix(loss=running_loss, acc=running_acc,\n",
    "                                    epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=model,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        # move model to cpu for sampling\n",
    "\n",
    "        nationalities = np.random.choice(np.arange(len(vectorizer.nationality_vocab)), replace=True, size=2)\n",
    "        model = model.cpu()\n",
    "        sampled_surnames = decode_samples(\n",
    "            sample_from_model(model, vectorizer, nationalities=nationalities),\n",
    "            vectorizer)\n",
    "\n",
    "        sample1 = \"{}->{}\".format(vectorizer.nationality_vocab.lookup_index(nationalities[0]),\n",
    "                                  sampled_surnames[0])\n",
    "        sample2 = \"{}->{}\".format(vectorizer.nationality_vocab.lookup_index(nationalities[1]),\n",
    "                                  sampled_surnames[1])\n",
    "        if batch_index % 25 == 0:\n",
    "            epoch_bar.set_postfix(sample1=sample1,\n",
    "                                  sample2=sample2)\n",
    "        # move model back to whichever device it should be on\n",
    "        model = model.to(args.device)\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce7c580-3a1e-4fde-b4c3-1f7ddc48ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled for Russian: \n",
      "-  Leghakov\n",
      "-  Faz\n",
      "-  Baner\n",
      "-  Ahramev\n",
      "-  Atdudamich\n"
     ]
    }
   ],
   "source": [
    "model = model.cpu()\n",
    "for index in [14]:\n",
    "    nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    print(\"Sampled for {}: \".format(nationality))\n",
    "    sampled_indices = sample_from_model(model, vectorizer,\n",
    "                                        nationalities=[index] * 5,\n",
    "                                        temperature=0.7)\n",
    "    for sampled_surname in decode_samples(sampled_indices, vectorizer):\n",
    "        print(\"-  \" + sampled_surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d676726-211e-4a61-9b5f-0c69154130ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
